{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12799389,"sourceType":"datasetVersion","datasetId":8092557},{"sourceId":12799442,"sourceType":"datasetVersion","datasetId":8092592},{"sourceId":567385,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":427507,"modelId":444513}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install yacs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T13:55:12.546407Z","iopub.execute_input":"2025-09-10T13:55:12.547014Z","iopub.status.idle":"2025-09-10T13:55:17.888797Z","shell.execute_reply.started":"2025-09-10T13:55:12.546985Z","shell.execute_reply":"2025-09-10T13:55:17.888162Z"}},"outputs":[{"name":"stdout","text":"Collecting yacs\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs) (6.0.2)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nInstalling collected packages: yacs\nSuccessfully installed yacs-0.1.8\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install warmup_scheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T13:55:59.975973Z","iopub.execute_input":"2025-09-10T13:55:59.976277Z","iopub.status.idle":"2025-09-10T13:56:05.438843Z","shell.execute_reply.started":"2025-09-10T13:55:59.976248Z","shell.execute_reply":"2025-09-10T13:56:05.438107Z"}},"outputs":[{"name":"stdout","text":"Collecting warmup_scheduler\n  Downloading warmup_scheduler-0.3.tar.gz (2.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: warmup_scheduler\n  Building wheel for warmup_scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for warmup_scheduler: filename=warmup_scheduler-0.3-py3-none-any.whl size=2971 sha256=9a981412c9bdd2fb647fda50ebab7f7cc1b3cec91ba7d1e279e3c9059dfa7476\n  Stored in directory: /root/.cache/pip/wheels/cc/5c/3b/6e5033100e0e4191383dad5c4279638a37f9791d1af9e1d85c\nSuccessfully built warmup_scheduler\nInstalling collected packages: warmup_scheduler\nSuccessfully installed warmup_scheduler-0.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ================================\n# IMPORTS ET CONFIGURATION\n# ================================\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os\nimport time\nimport numpy as np\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nimport sys\n\nprint(\"✅ Imports terminés\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T13:56:16.824509Z","iopub.execute_input":"2025-09-10T13:56:16.825317Z","iopub.status.idle":"2025-09-10T13:56:27.751466Z","shell.execute_reply.started":"2025-09-10T13:56:16.825283Z","shell.execute_reply":"2025-09-10T13:56:27.750802Z"}},"outputs":[{"name":"stdout","text":"✅ Imports terminés\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ================================\n# FONCTIONS DE PERTE\n# ================================\n\nclass CharbonnierLoss(nn.Module):\n    \"\"\"Perte Charbonnier (L1 smooth)\"\"\"\n    def __init__(self, eps=1e-3):\n        super(CharbonnierLoss, self).__init__()\n        self.eps = eps\n\n    def forward(self, x, y):\n        diff = x - y\n        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps*self.eps)))\n        return loss\n\nclass EdgeLoss(nn.Module):\n    \"\"\"Perte basée sur les contours (Laplacien)\"\"\"\n    def __init__(self):\n        super(EdgeLoss, self).__init__()\n        k = torch.Tensor([[.05, .25, .4, .25, .05]])\n        self.kernel = torch.matmul(k.t(),k).unsqueeze(0).repeat(3,1,1,1)\n        if torch.cuda.is_available():\n            self.kernel = self.kernel.cuda()\n        self.loss = CharbonnierLoss()\n\n    def conv_gauss(self, img):\n        n_channels, _, kw, kh = self.kernel.shape\n        img = F.pad(img, (kw//2, kh//2, kw//2, kh//2), mode='replicate')\n        return F.conv2d(img, self.kernel, groups=n_channels)\n\n    def laplacian_kernel(self, current):\n        filtered    = self.conv_gauss(current)\n        down        = filtered[:,:,::2,::2]\n        new_filter  = torch.zeros_like(filtered)\n        new_filter[:,:,::2,::2] = down*4\n        filtered    = self.conv_gauss(new_filter)\n        diff = current - filtered\n        return diff\n\n    def forward(self, x, y):\n        loss1 = self.loss(x, y)\n        loss2 = self.loss(self.laplacian_kernel(x), self.laplacian_kernel(y))\n        return loss1 + 0.1 * loss2\n\nprint(\"✅ Fonctions de perte définies\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T13:56:34.796973Z","iopub.execute_input":"2025-09-10T13:56:34.797351Z","iopub.status.idle":"2025-09-10T13:56:34.807071Z","shell.execute_reply.started":"2025-09-10T13:56:34.797330Z","shell.execute_reply":"2025-09-10T13:56:34.806159Z"}},"outputs":[{"name":"stdout","text":"✅ Fonctions de perte définies\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ================================\n# ARCHITECTURE MPRNet LÉGÈRE\n# ================================\n\nclass ChannelAttentionBlock(nn.Module):\n    \"\"\"Channel Attention Block simplifié\"\"\"\n    def __init__(self, channels, reduction=4):\n        super(ChannelAttentionBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channels, channels // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channels // reduction, channels, bias=False)\n        )\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        avg_out = self.fc(self.avg_pool(x).view(b, c))\n        max_out = self.fc(self.max_pool(x).view(b, c))\n        out = avg_out + max_out\n        return x * self.sigmoid(out).view(b, c, 1, 1)\n\nclass LightweightImprovedMPRNet(nn.Module):\n    \"\"\"MPRNet ultra-léger pour Kaggle\"\"\"\n    def __init__(self, n_feat=8, scale_unetfeats=4, scale_orsnetfeats=2, num_cab=1):\n        super(LightweightImprovedMPRNet, self).__init__()\n        \n        # Paramètres ultra-minimaux\n        self.n_feat = n_feat\n        self.scale_unetfeats = scale_unetfeats\n        self.scale_orsnetfeats = scale_orsnetfeats\n        self.num_cab = num_cab\n        \n        # Shallow feature extraction\n        self.shallow_feat = nn.Sequential(\n            nn.Conv2d(3, n_feat, kernel_size=3, padding=1),\n            nn.Conv2d(n_feat, n_feat, kernel_size=3, padding=1)\n        )\n        \n        # Stage 1 - Encoder-Decoder\n        self.stage1_encoder = nn.Sequential(\n            nn.Conv2d(n_feat, n_feat*2, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(n_feat*2, n_feat*4, 4, 2, 1),\n            nn.LeakyReLU(0.2)\n        )\n        \n        self.stage1_decoder = nn.Sequential(\n            nn.ConvTranspose2d(n_feat*4, n_feat*2, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.ConvTranspose2d(n_feat*2, n_feat, 4, 2, 1),\n            nn.LeakyReLU(0.2)\n        )\n        \n        # Stage 2 - Original Resolution\n        self.stage2 = nn.Sequential(\n            nn.Conv2d(n_feat, n_feat, 3, 1, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(n_feat, n_feat, 3, 1, 1),\n            nn.LeakyReLU(0.2)\n        )\n        \n        # Channel Attention\n        self.cab = ChannelAttentionBlock(n_feat)\n        \n        # Output\n        self.output = nn.Conv2d(n_feat, 3, 3, 1, 1)\n        \n    def forward(self, x):\n        # Shallow features\n        shallow = self.shallow_feat(x)\n        \n        # Stage 1\n        stage1_enc = self.stage1_encoder(shallow)\n        stage1_dec = self.stage1_decoder(stage1_enc)\n        \n        # Fusion\n        fused = shallow + stage1_dec\n        \n        # Stage 2\n        stage2_out = self.stage2(fused)\n        \n        # Attention\n        attended = self.cab(stage2_out)\n        \n        # Output\n        output = self.output(attended)\n        \n        return output\n\nprint(\"✅ Architecture MPRNet légère définie\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T13:56:39.307223Z","iopub.execute_input":"2025-09-10T13:56:39.307524Z","iopub.status.idle":"2025-09-10T13:56:39.319360Z","shell.execute_reply.started":"2025-09-10T13:56:39.307499Z","shell.execute_reply":"2025-09-10T13:56:39.318497Z"}},"outputs":[{"name":"stdout","text":"✅ Architecture MPRNet légère définie\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ================================\n# FONCTIONS UTILITAIRES\n# ================================\n\ndef calculate_psnr_ssim(pred, target):\n    \"\"\"Calculer PSNR et SSIM pour un batch d'images\"\"\"\n    pred_np = pred.detach().cpu().numpy()\n    target_np = target.detach().cpu().numpy()\n    \n    psnr_values = []\n    ssim_values = []\n    \n    for i in range(pred_np.shape[0]):\n        # Convertir de [-1, 1] à [0, 1]\n        pred_img = (pred_np[i].transpose(1, 2, 0) + 1) / 2\n        target_img = (target_np[i].transpose(1, 2, 0) + 1) / 2\n        \n        # Clamper les valeurs\n        pred_img = np.clip(pred_img, 0, 1)\n        target_img = np.clip(target_img, 0, 1)\n        \n        # Calculer PSNR\n        psnr_val = psnr(target_img, pred_img, data_range=1.0)\n        psnr_values.append(psnr_val)\n        \n        # Calculer SSIM avec paramètres adaptés\n        try:\n            # Déterminer la taille de fenêtre appropriée\n            min_dim = min(pred_img.shape[0], pred_img.shape[1])\n            win_size = min(7, min_dim) if min_dim >= 7 else min_dim\n            if win_size % 2 == 0:\n                win_size -= 1  # SSIM nécessite une taille impaire\n            \n            # Calculer SSIM avec channel_axis au lieu de multichannel\n            ssim_val = ssim(target_img, pred_img, \n                           win_size=win_size, \n                           channel_axis=2, \n                           data_range=1.0)\n        except Exception as e:\n            print(f\"Erreur SSIM pour image {i}: {e}\")\n            ssim_val = 0.0  # Valeur par défaut en cas d'erreur\n        \n        ssim_values.append(ssim_val)\n    \n    return np.mean(psnr_values), np.mean(ssim_values)\n\nprint(\"✅ Fonctions utilitaires définies\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T13:56:44.597847Z","iopub.execute_input":"2025-09-10T13:56:44.598161Z","iopub.status.idle":"2025-09-10T13:56:44.606443Z","shell.execute_reply.started":"2025-09-10T13:56:44.598128Z","shell.execute_reply":"2025-09-10T13:56:44.605427Z"}},"outputs":[{"name":"stdout","text":"✅ Fonctions utilitaires définies\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ================================\n# DATASET GOPRO POUR KAGGLE\n# ================================\n\nclass GoProDataset(Dataset):\n    \"\"\"Dataset GoPro pour Kaggle\"\"\"\n    def __init__(self, root_dir, patch_size=64, is_training=True):\n        self.root_dir = root_dir\n        self.patch_size = patch_size\n        self.is_training = is_training\n        \n        # Chemins des dossiers\n        self.input_dir = os.path.join(root_dir, 'input')\n        self.target_dir = os.path.join(root_dir, 'target')\n        \n        # Vérifier que les dossiers existent\n        if not os.path.exists(self.input_dir):\n            raise ValueError(f\"Dossier input non trouvé: {self.input_dir}\")\n        if not os.path.exists(self.target_dir):\n            raise ValueError(f\"Dossier target non trouvé: {self.target_dir}\")\n        \n        # Lister les fichiers\n        self.input_files = sorted([f for f in os.listdir(self.input_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n        self.target_files = sorted([f for f in os.listdir(self.target_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n        \n        # Vérifier que les fichiers correspondent\n        if len(self.input_files) != len(self.target_files):\n            print(f\"⚠️  Nombre de fichiers différent: input={len(self.input_files)}, target={len(self.target_files)}\")\n        \n        # Prendre le minimum pour éviter les erreurs\n        self.num_files = min(len(self.input_files), len(self.target_files))\n        \n        # Transformations\n        if is_training:\n            self.transform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n            ])\n        else:\n            self.transform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n            ])\n    \n    def __len__(self):\n        return self.num_files\n    \n    def __getitem__(self, idx):\n        # Charger les images\n        input_path = os.path.join(self.input_dir, self.input_files[idx])\n        target_path = os.path.join(self.target_dir, self.target_files[idx])\n        \n        try:\n            input_img = Image.open(input_path).convert('RGB')\n            target_img = Image.open(target_path).convert('RGB')\n            \n            # Redimensionner si nécessaire\n            if self.patch_size > 0:\n                input_img = input_img.resize((self.patch_size, self.patch_size), Image.LANCZOS)\n                target_img = target_img.resize((self.patch_size, self.patch_size), Image.LANCZOS)\n            \n            # Appliquer les transformations\n            input_tensor = self.transform(input_img)\n            target_tensor = self.transform(target_img)\n            \n            return input_tensor, target_tensor\n            \n        except Exception as e:\n            print(f\"Erreur lors du chargement des images {idx}: {e}\")\n            # Retourner des images vides en cas d'erreur\n            empty_img = torch.zeros(3, self.patch_size, self.patch_size)\n            return empty_img, empty_img\n\nprint(\"✅ Dataset GoPro pour Kaggle défini\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T13:56:51.362913Z","iopub.execute_input":"2025-09-10T13:56:51.363191Z","iopub.status.idle":"2025-09-10T13:56:51.374427Z","shell.execute_reply.started":"2025-09-10T13:56:51.363171Z","shell.execute_reply":"2025-09-10T13:56:51.373798Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset GoPro pour Kaggle défini\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ================================\n# FONCTION D'ENTRAÎNEMENT PRINCIPALE\n# ================================\n\ndef train_improved_model_100_epochs():\n    \"\"\"Entraînement MPRNet amélioré avec 100 époques\"\"\"\n    \n    print(\"🚀 ENTRAÎNEMENT MPRNet AMÉLIORÉ - 100 ÉPOQUES\")\n    print(\"=\" * 60)\n    \n    # Configuration\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"🖥️  Device: {device}\")\n    \n    # Configuration optimisée pour Kaggle\n    batch_size = 16\n    num_epochs = 100\n    lr_initial = 2e-4\n    lr_min = 1e-6\n    train_ps = 256\n    val_ps = 256\n    \n    # Modèle optimisé pour grandes images\n    model = LightweightImprovedMPRNet(\n        n_feat=16,  # Légèrement augmenté pour 256x256\n        scale_unetfeats=8,\n        scale_orsnetfeats=4,\n        num_cab=2\n    ).to(device)\n    \n    # Multi-GPU si disponible\n    if torch.cuda.device_count() > 1 and batch_size > 1:\n        print(f\"🚀 Utilisation de {torch.cuda.device_count()} GPU\")\n        model = torch.nn.DataParallel(model)\n    \n    # Optimizer et scheduler\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr_initial, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=lr_min)\n    \n    # Loss functions\n    charb_loss = CharbonnierLoss().to(device)\n    edge_loss = EdgeLoss().to(device)\n    \n    def combined_loss(pred, target):\n        charb = charb_loss(pred, target)\n        edge = edge_loss(pred, target)\n        return charb + 0.05 * edge\n    \n    # Datasets GoPro\n    train_dir = '/kaggle/input/gopro-training'\n    val_dir = '/kaggle/input/gopro-training'\n    \n    print(f\"📁 Chargement des datasets GoPro...\")\n    print(f\"   Train: {train_dir}\")\n    print(f\"   Val: {val_dir}\")\n    \n    train_dataset = GoProDataset(train_dir, patch_size=train_ps, is_training=True)\n    val_dataset = GoProDataset(val_dir, patch_size=val_ps, is_training=False)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    \n    print(f\"📈 Dataset GoPro chargé:\")\n    print(f\"   Train: {len(train_dataset)} images\")\n    print(f\"   Val: {len(val_dataset)} images\")\n    print(f\"🔧 Batch size: {batch_size}, Epochs: {num_epochs}\")\n    print(f\"🖼️  Patch size: {train_ps}x{train_ps}\")\n    \n    # Variables de suivi\n    best_psnr = 0.0\n    best_ssim = 0.0\n    best_epoch = 0\n    train_losses = []\n    val_psnrs = []\n    val_ssims = []\n    validation_epochs = []\n    \n    print(f\"\\n🏋️ DÉBUT DE L'ENTRAÎNEMENT - VALIDATION CHAQUE 10 ÉPOQUES\")\n    print(\"=\" * 60)\n    \n    start_time = time.time()\n    \n    for epoch in range(num_epochs):\n        # ===== ENTRAÎNEMENT =====\n        model.train()\n        epoch_loss = 0.0\n        num_batches = 0\n        \n        for batch_idx, (input_imgs, target_imgs) in enumerate(train_loader):\n            input_imgs = input_imgs.to(device, non_blocking=True)\n            target_imgs = target_imgs.to(device, non_blocking=True)\n            \n            # Forward pass\n            optimizer.zero_grad()\n            output = model(input_imgs)\n            pred_imgs = output[0] if isinstance(output, list) else output\n            \n            # Loss\n            loss = combined_loss(pred_imgs, target_imgs)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n            num_batches += 1\n            \n            # Affichage progressif\n            if batch_idx % 50 == 0:\n                print(f\"Epoch {epoch+1:3d}/{num_epochs} | Batch {batch_idx:4d} | Loss: {loss.item():.6f}\")\n        \n        avg_loss = epoch_loss / num_batches\n        train_losses.append(avg_loss)\n        \n        # ===== VALIDATION (toutes les 10 époques) =====\n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            model.eval()\n            val_psnr_sum = 0.0\n            val_ssim_sum = 0.0\n            val_batches = 0\n            \n            print(f\"\\n🔍 VALIDATION - Époque {epoch+1}\")\n            print(\"-\" * 50)\n            \n            with torch.no_grad():\n                for input_imgs, target_imgs in val_loader:\n                    input_imgs = input_imgs.to(device, non_blocking=True)\n                    target_imgs = target_imgs.to(device, non_blocking=True)\n                    \n                    output = model(input_imgs)\n                    pred_imgs = output[0] if isinstance(output, list) else output\n                    \n                    # Calculer PSNR et SSIM\n                    psnr_val, ssim_val = calculate_psnr_ssim(pred_imgs, target_imgs)\n                    val_psnr_sum += psnr_val\n                    val_ssim_sum += ssim_val\n                    val_batches += 1\n            \n            avg_psnr = val_psnr_sum / val_batches\n            avg_ssim = val_ssim_sum / val_batches\n            \n            val_psnrs.append(avg_psnr)\n            val_ssims.append(avg_ssim)\n            validation_epochs.append(epoch + 1)\n            \n            # Vérifier si c'est le meilleur modèle\n            is_best = avg_psnr > best_psnr\n            if is_best:\n                best_psnr = avg_psnr\n                best_ssim = avg_ssim\n                best_epoch = epoch + 1\n                \n                # Sauvegarder le meilleur modèle\n                torch.save({\n                    'epoch': epoch + 1,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'best_psnr': best_psnr,\n                    'best_ssim': best_ssim,\n                    'best_epoch': best_epoch,\n                    'train_losses': train_losses,\n                    'val_psnrs': val_psnrs,\n                    'val_ssims': val_ssims,\n                    'validation_epochs': validation_epochs\n                }, 'best_mprnet_model.pth')\n                \n                print(f\"🏆 NOUVEAU MEILLEUR MODÈLE!\")\n                print(f\"   PSNR: {best_psnr:.4f} dB (Époque {best_epoch})\")\n                print(f\"   SSIM: {best_ssim:.4f}\")\n                print(f\"   �� Sauvegardé: best_mprnet_model.pth\")\n            else:\n                print(f\"📊 Résultats actuels:\")\n                print(f\"   PSNR: {avg_psnr:.4f} dB (Meilleur: {best_psnr:.4f} @ Époque {best_epoch})\")\n                print(f\"   SSIM: {avg_ssim:.4f} (Meilleur: {best_ssim:.4f})\")\n            \n            # Affichage détaillé\n            current_lr = optimizer.param_groups[0]['lr']\n            elapsed_time = time.time() - start_time\n            \n            print(f\"\\n📊 DÉTAILS DE L'ÉPOQUE {epoch+1:3d}/{num_epochs}\")\n            print(f\"   Loss d'entraînement: {avg_loss:.6f}\")\n            print(f\"   Learning rate: {current_lr:.2e}\")\n            print(f\"   Temps écoulé: {elapsed_time/60:.1f} minutes\")\n            print(f\"   Progrès: {((epoch+1)/num_epochs)*100:.1f}%\")\n            print(\"=\" * 60)\n        \n        # Mettre à jour le scheduler\n        scheduler.step()\n        \n        # Nettoyer la mémoire\n        torch.cuda.empty_cache()\n    \n    # ===== SAUVEGARDE FINALE =====\n    total_time = time.time() - start_time\n    \n    # Sauvegarder le modèle final\n    final_model_path = 'mprnet_final_model.pth'\n    torch.save({\n        'epoch': num_epochs,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'best_psnr': best_psnr,\n        'best_ssim': best_ssim,\n        'best_epoch': best_epoch,\n        'train_losses': train_losses,\n        'val_psnrs': val_psnrs,\n        'val_ssims': val_ssims,\n        'validation_epochs': validation_epochs,\n        'config': {\n            'batch_size': batch_size,\n            'num_epochs': num_epochs,\n            'lr_initial': lr_initial,\n            'lr_min': lr_min,\n            'train_ps': train_ps,\n            'val_ps': val_ps\n        }\n    }, final_model_path)\n    \n    # ===== RÉSULTATS FINAUX =====\n    print(\"\\n🎉 ENTRAÎNEMENT TERMINÉ!\")\n    print(\"=\" * 60)\n    print(f\"⏱️  Temps total: {total_time/60:.1f} minutes\")\n    print(f\"🏆 MEILLEUR MODÈLE (Époque {best_epoch}):\")\n    print(f\"   PSNR: {best_psnr:.4f} dB\")\n    print(f\"   SSIM: {best_ssim:.4f}\")\n    print(f\"💾 Fichiers sauvegardés:\")\n    print(f\"   - Meilleur modèle: best_mprnet_model.pth\")\n    print(f\"   - Modèle final: {final_model_path}\")\n    \n    # Statistiques d'entraînement\n    print(f\"\\n📊 STATISTIQUES D'ENTRAÎNEMENT:\")\n    print(f\"   Époques totales: {num_epochs}\")\n    print(f\"   Validations effectuées: {len(validation_epochs)}\")\n    print(f\"   Époques de validation: {validation_epochs}\")\n    print(f\"   Batches par époque: {len(train_loader)}\")\n    print(f\"   Total de batches: {num_epochs * len(train_loader)}\")\n    print(f\"   Loss finale: {train_losses[-1]:.6f}\")\n    \n    # Évolution des performances\n    print(f\"\\n📈 ÉVOLUTION DES PERFORMANCES:\")\n    for i, (ep, psnr, ssim) in enumerate(zip(validation_epochs, val_psnrs, val_ssims)):\n        marker = \"🏆\" if ep == best_epoch else \"  \"\n        print(f\"   {marker} Époque {ep:3d}: PSNR {psnr:.4f} dB, SSIM {ssim:.4f}\")\n    \n    return model, best_psnr, best_ssim, best_epoch\n\nprint(\"✅ Fonction d'entraînement définie\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T13:56:56.025273Z","iopub.execute_input":"2025-09-10T13:56:56.025574Z","iopub.status.idle":"2025-09-10T13:56:56.048108Z","shell.execute_reply.started":"2025-09-10T13:56:56.025551Z","shell.execute_reply":"2025-09-10T13:56:56.047269Z"}},"outputs":[{"name":"stdout","text":"✅ Fonction d'entraînement définie\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ================================\n# LANCEMENT DE L'ENTRAÎNEMENT\n# ================================\n\nprint(\"🎯 LANCEMENT DE L'ENTRAÎNEMENT MPRNet AMÉLIORÉ\")\nprint(\"=\" * 60)\n\n# Lancer l'entraînement\ntrained_model, best_psnr, best_ssim, best_epoch = train_improved_model_100_epochs()\n\nprint(f\"\\n✅ Entraînement terminé avec succès!\")\nprint(f\"🏆 Meilleures performances: PSNR {best_psnr:.4f} dB, SSIM {best_ssim:.4f}\")\nprint(f\"🎯 Meilleur modèle obtenu à l'époque {best_epoch}\")\nprint(f\"📁 Modèles sauvegardés pour téléchargement\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T13:57:03.119204Z","iopub.execute_input":"2025-09-10T13:57:03.119834Z","iopub.status.idle":"2025-09-10T17:10:08.354468Z","shell.execute_reply.started":"2025-09-10T13:57:03.119792Z","shell.execute_reply":"2025-09-10T17:10:08.353643Z"}},"outputs":[{"name":"stdout","text":"🎯 LANCEMENT DE L'ENTRAÎNEMENT MPRNet AMÉLIORÉ\n============================================================\n🚀 ENTRAÎNEMENT MPRNet AMÉLIORÉ - 100 ÉPOQUES\n============================================================\n🖥️  Device: cuda\n🚀 Utilisation de 2 GPU\n📁 Chargement des datasets GoPro...\n   Train: /kaggle/input/gopro-training\n   Val: /kaggle/input/gopro-training\n📈 Dataset GoPro chargé:\n   Train: 2103 images\n   Val: 2103 images\n🔧 Batch size: 16, Epochs: 100\n🖼️  Patch size: 256x256\n\n🏋️ DÉBUT DE L'ENTRAÎNEMENT - VALIDATION CHAQUE 10 ÉPOQUES\n============================================================\nEpoch   1/100 | Batch    0 | Loss: 0.428007\nEpoch   1/100 | Batch   50 | Loss: 0.159673\nEpoch   1/100 | Batch  100 | Loss: 0.133924\n\n🔍 VALIDATION - Époque 1\n--------------------------------------------------\n🏆 NOUVEAU MEILLEUR MODÈLE!\n   PSNR: 20.9043 dB (Époque 1)\n   SSIM: 0.6524\n   �� Sauvegardé: best_mprnet_model.pth\n\n📊 DÉTAILS DE L'ÉPOQUE   1/100\n   Loss d'entraînement: 0.193564\n   Learning rate: 2.00e-04\n   Temps écoulé: 4.3 minutes\n   Progrès: 1.0%\n============================================================\nEpoch   2/100 | Batch    0 | Loss: 0.140871\nEpoch   2/100 | Batch   50 | Loss: 0.128288\nEpoch   2/100 | Batch  100 | Loss: 0.105572\nEpoch   3/100 | Batch    0 | Loss: 0.101632\nEpoch   3/100 | Batch   50 | Loss: 0.091886\nEpoch   3/100 | Batch  100 | Loss: 0.083035\nEpoch   4/100 | Batch    0 | Loss: 0.091612\nEpoch   4/100 | Batch   50 | Loss: 0.082516\nEpoch   4/100 | Batch  100 | Loss: 0.084451\nEpoch   5/100 | Batch    0 | Loss: 0.084689\nEpoch   5/100 | Batch   50 | Loss: 0.085009\nEpoch   5/100 | Batch  100 | Loss: 0.076924\nEpoch   6/100 | Batch    0 | Loss: 0.075428\nEpoch   6/100 | Batch   50 | Loss: 0.080971\nEpoch   6/100 | Batch  100 | Loss: 0.088491\nEpoch   7/100 | Batch    0 | Loss: 0.073968\nEpoch   7/100 | Batch   50 | Loss: 0.055945\nEpoch   7/100 | Batch  100 | Loss: 0.077785\nEpoch   8/100 | Batch    0 | Loss: 0.068744\nEpoch   8/100 | Batch   50 | Loss: 0.058304\nEpoch   8/100 | Batch  100 | Loss: 0.069158\nEpoch   9/100 | Batch    0 | Loss: 0.066034\nEpoch   9/100 | Batch   50 | Loss: 0.067771\nEpoch   9/100 | Batch  100 | Loss: 0.061045\nEpoch  10/100 | Batch    0 | Loss: 0.066276\nEpoch  10/100 | Batch   50 | Loss: 0.069548\nEpoch  10/100 | Batch  100 | Loss: 0.065527\n\n🔍 VALIDATION - Époque 10\n--------------------------------------------------\n🏆 NOUVEAU MEILLEUR MODÈLE!\n   PSNR: 26.1906 dB (Époque 10)\n   SSIM: 0.8273\n   �� Sauvegardé: best_mprnet_model.pth\n\n📊 DÉTAILS DE L'ÉPOQUE  10/100\n   Loss d'entraînement: 0.067108\n   Learning rate: 1.96e-04\n   Temps écoulé: 21.7 minutes\n   Progrès: 10.0%\n============================================================\nEpoch  11/100 | Batch    0 | Loss: 0.076317\nEpoch  11/100 | Batch   50 | Loss: 0.068796\nEpoch  11/100 | Batch  100 | Loss: 0.079770\nEpoch  12/100 | Batch    0 | Loss: 0.059953\nEpoch  12/100 | Batch   50 | Loss: 0.059395\nEpoch  12/100 | Batch  100 | Loss: 0.062757\nEpoch  13/100 | Batch    0 | Loss: 0.050348\nEpoch  13/100 | Batch   50 | Loss: 0.064133\nEpoch  13/100 | Batch  100 | Loss: 0.066710\nEpoch  14/100 | Batch    0 | Loss: 0.056890\nEpoch  14/100 | Batch   50 | Loss: 0.073631\nEpoch  14/100 | Batch  100 | Loss: 0.064961\nEpoch  15/100 | Batch    0 | Loss: 0.065523\nEpoch  15/100 | Batch   50 | Loss: 0.072575\nEpoch  15/100 | Batch  100 | Loss: 0.062671\nEpoch  16/100 | Batch    0 | Loss: 0.059963\nEpoch  16/100 | Batch   50 | Loss: 0.049485\nEpoch  16/100 | Batch  100 | Loss: 0.082197\nEpoch  17/100 | Batch    0 | Loss: 0.067381\nEpoch  17/100 | Batch   50 | Loss: 0.048142\nEpoch  17/100 | Batch  100 | Loss: 0.066759\nEpoch  18/100 | Batch    0 | Loss: 0.065873\nEpoch  18/100 | Batch   50 | Loss: 0.068494\nEpoch  18/100 | Batch  100 | Loss: 0.064633\nEpoch  19/100 | Batch    0 | Loss: 0.062207\nEpoch  19/100 | Batch   50 | Loss: 0.054793\nEpoch  19/100 | Batch  100 | Loss: 0.053356\nEpoch  20/100 | Batch    0 | Loss: 0.068165\nEpoch  20/100 | Batch   50 | Loss: 0.068452\nEpoch  20/100 | Batch  100 | Loss: 0.066989\n\n🔍 VALIDATION - Époque 20\n--------------------------------------------------\n🏆 NOUVEAU MEILLEUR MODÈLE!\n   PSNR: 26.9446 dB (Époque 20)\n   SSIM: 0.8392\n   �� Sauvegardé: best_mprnet_model.pth\n\n📊 DÉTAILS DE L'ÉPOQUE  20/100\n   Loss d'entraînement: 0.062154\n   Learning rate: 1.83e-04\n   Temps écoulé: 40.7 minutes\n   Progrès: 20.0%\n============================================================\nEpoch  21/100 | Batch    0 | Loss: 0.050805\nEpoch  21/100 | Batch   50 | Loss: 0.054798\nEpoch  21/100 | Batch  100 | Loss: 0.059623\nEpoch  22/100 | Batch    0 | Loss: 0.071520\nEpoch  22/100 | Batch   50 | Loss: 0.060584\nEpoch  22/100 | Batch  100 | Loss: 0.067697\nEpoch  23/100 | Batch    0 | Loss: 0.079395\nEpoch  23/100 | Batch   50 | Loss: 0.069464\nEpoch  23/100 | Batch  100 | Loss: 0.057541\nEpoch  24/100 | Batch    0 | Loss: 0.063094\nEpoch  24/100 | Batch   50 | Loss: 0.061862\nEpoch  24/100 | Batch  100 | Loss: 0.059287\nEpoch  25/100 | Batch    0 | Loss: 0.045449\nEpoch  25/100 | Batch   50 | Loss: 0.057853\nEpoch  25/100 | Batch  100 | Loss: 0.061531\nEpoch  26/100 | Batch    0 | Loss: 0.059815\nEpoch  26/100 | Batch   50 | Loss: 0.056976\nEpoch  26/100 | Batch  100 | Loss: 0.060116\nEpoch  27/100 | Batch    0 | Loss: 0.061175\nEpoch  27/100 | Batch   50 | Loss: 0.044407\nEpoch  27/100 | Batch  100 | Loss: 0.073205\nEpoch  28/100 | Batch    0 | Loss: 0.053660\nEpoch  28/100 | Batch   50 | Loss: 0.068335\nEpoch  28/100 | Batch  100 | Loss: 0.067961\nEpoch  29/100 | Batch    0 | Loss: 0.054400\nEpoch  29/100 | Batch   50 | Loss: 0.045425\nEpoch  29/100 | Batch  100 | Loss: 0.066816\nEpoch  30/100 | Batch    0 | Loss: 0.054809\nEpoch  30/100 | Batch   50 | Loss: 0.069678\nEpoch  30/100 | Batch  100 | Loss: 0.064635\n\n🔍 VALIDATION - Époque 30\n--------------------------------------------------\n🏆 NOUVEAU MEILLEUR MODÈLE!\n   PSNR: 27.2688 dB (Époque 30)\n   SSIM: 0.8434\n   �� Sauvegardé: best_mprnet_model.pth\n\n📊 DÉTAILS DE L'ÉPOQUE  30/100\n   Loss d'entraînement: 0.060227\n   Learning rate: 1.61e-04\n   Temps écoulé: 59.7 minutes\n   Progrès: 30.0%\n============================================================\nEpoch  31/100 | Batch    0 | Loss: 0.047301\nEpoch  31/100 | Batch   50 | Loss: 0.056660\nEpoch  31/100 | Batch  100 | Loss: 0.059237\nEpoch  32/100 | Batch    0 | Loss: 0.059568\nEpoch  32/100 | Batch   50 | Loss: 0.067304\nEpoch  32/100 | Batch  100 | Loss: 0.063008\nEpoch  33/100 | Batch    0 | Loss: 0.062539\nEpoch  33/100 | Batch   50 | Loss: 0.066972\nEpoch  33/100 | Batch  100 | Loss: 0.054604\nEpoch  34/100 | Batch    0 | Loss: 0.055291\nEpoch  34/100 | Batch   50 | Loss: 0.068617\nEpoch  34/100 | Batch  100 | Loss: 0.063755\nEpoch  35/100 | Batch    0 | Loss: 0.066113\nEpoch  35/100 | Batch   50 | Loss: 0.056930\nEpoch  35/100 | Batch  100 | Loss: 0.070857\nEpoch  36/100 | Batch    0 | Loss: 0.055504\nEpoch  36/100 | Batch   50 | Loss: 0.060695\nEpoch  36/100 | Batch  100 | Loss: 0.059492\nEpoch  37/100 | Batch    0 | Loss: 0.077347\nEpoch  37/100 | Batch   50 | Loss: 0.050992\nEpoch  37/100 | Batch  100 | Loss: 0.047536\nEpoch  38/100 | Batch    0 | Loss: 0.057606\nEpoch  38/100 | Batch   50 | Loss: 0.050384\nEpoch  38/100 | Batch  100 | Loss: 0.064347\nEpoch  39/100 | Batch    0 | Loss: 0.058092\nEpoch  39/100 | Batch   50 | Loss: 0.050720\nEpoch  39/100 | Batch  100 | Loss: 0.054130\nEpoch  40/100 | Batch    0 | Loss: 0.061588\nEpoch  40/100 | Batch   50 | Loss: 0.062337\nEpoch  40/100 | Batch  100 | Loss: 0.068143\n\n🔍 VALIDATION - Époque 40\n--------------------------------------------------\n🏆 NOUVEAU MEILLEUR MODÈLE!\n   PSNR: 27.4531 dB (Époque 40)\n   SSIM: 0.8458\n   �� Sauvegardé: best_mprnet_model.pth\n\n📊 DÉTAILS DE L'ÉPOQUE  40/100\n   Loss d'entraînement: 0.059109\n   Learning rate: 1.34e-04\n   Temps écoulé: 78.6 minutes\n   Progrès: 40.0%\n============================================================\nEpoch  41/100 | Batch    0 | Loss: 0.060493\nEpoch  41/100 | Batch   50 | Loss: 0.057965\nEpoch  41/100 | Batch  100 | Loss: 0.046717\nEpoch  42/100 | Batch    0 | Loss: 0.068798\nEpoch  42/100 | Batch   50 | Loss: 0.050123\nEpoch  42/100 | Batch  100 | Loss: 0.061488\nEpoch  43/100 | Batch    0 | Loss: 0.051124\nEpoch  43/100 | Batch   50 | Loss: 0.058886\nEpoch  43/100 | Batch  100 | Loss: 0.060648\nEpoch  44/100 | Batch    0 | Loss: 0.046120\nEpoch  44/100 | Batch   50 | Loss: 0.068297\nEpoch  44/100 | Batch  100 | Loss: 0.053310\nEpoch  45/100 | Batch    0 | Loss: 0.046481\nEpoch  45/100 | Batch   50 | Loss: 0.052560\nEpoch  45/100 | Batch  100 | Loss: 0.052383\nEpoch  46/100 | Batch    0 | Loss: 0.052172\nEpoch  46/100 | Batch   50 | Loss: 0.076787\nEpoch  46/100 | Batch  100 | Loss: 0.054354\nEpoch  47/100 | Batch    0 | Loss: 0.057438\nEpoch  47/100 | Batch   50 | Loss: 0.051830\nEpoch  47/100 | Batch  100 | Loss: 0.064638\nEpoch  48/100 | Batch    0 | Loss: 0.069426\nEpoch  48/100 | Batch   50 | Loss: 0.061295\nEpoch  48/100 | Batch  100 | Loss: 0.067302\nEpoch  49/100 | Batch    0 | Loss: 0.067401\nEpoch  49/100 | Batch   50 | Loss: 0.054627\nEpoch  49/100 | Batch  100 | Loss: 0.054410\nEpoch  50/100 | Batch    0 | Loss: 0.055632\nEpoch  50/100 | Batch   50 | Loss: 0.054170\nEpoch  50/100 | Batch  100 | Loss: 0.054796\n\n🔍 VALIDATION - Époque 50\n--------------------------------------------------\n🏆 NOUVEAU MEILLEUR MODÈLE!\n   PSNR: 27.5524 dB (Époque 50)\n   SSIM: 0.8470\n   �� Sauvegardé: best_mprnet_model.pth\n\n📊 DÉTAILS DE L'ÉPOQUE  50/100\n   Loss d'entraînement: 0.058552\n   Learning rate: 1.04e-04\n   Temps écoulé: 97.7 minutes\n   Progrès: 50.0%\n============================================================\nEpoch  51/100 | Batch    0 | Loss: 0.060863\nEpoch  51/100 | Batch   50 | Loss: 0.052702\nEpoch  51/100 | Batch  100 | Loss: 0.044126\nEpoch  52/100 | Batch    0 | Loss: 0.052351\nEpoch  52/100 | Batch   50 | Loss: 0.064274\nEpoch  52/100 | Batch  100 | Loss: 0.054944\nEpoch  53/100 | Batch    0 | Loss: 0.039959\nEpoch  53/100 | Batch   50 | Loss: 0.050626\nEpoch  53/100 | Batch  100 | Loss: 0.062443\nEpoch  54/100 | Batch    0 | Loss: 0.054428\nEpoch  54/100 | Batch   50 | Loss: 0.061459\nEpoch  54/100 | Batch  100 | Loss: 0.060097\nEpoch  55/100 | Batch    0 | Loss: 0.065869\nEpoch  55/100 | Batch   50 | Loss: 0.051478\nEpoch  55/100 | Batch  100 | Loss: 0.053020\nEpoch  56/100 | Batch    0 | Loss: 0.065272\nEpoch  56/100 | Batch   50 | Loss: 0.050417\nEpoch  56/100 | Batch  100 | Loss: 0.055983\nEpoch  57/100 | Batch    0 | Loss: 0.044551\nEpoch  57/100 | Batch   50 | Loss: 0.055742\nEpoch  57/100 | Batch  100 | Loss: 0.074119\nEpoch  58/100 | Batch    0 | Loss: 0.056068\nEpoch  58/100 | Batch   50 | Loss: 0.059227\nEpoch  58/100 | Batch  100 | Loss: 0.050557\nEpoch  59/100 | Batch    0 | Loss: 0.050793\nEpoch  59/100 | Batch   50 | Loss: 0.053065\nEpoch  59/100 | Batch  100 | Loss: 0.047877\nEpoch  60/100 | Batch    0 | Loss: 0.047305\nEpoch  60/100 | Batch   50 | Loss: 0.049523\nEpoch  60/100 | Batch  100 | Loss: 0.054097\n\n🔍 VALIDATION - Époque 60\n--------------------------------------------------\n🏆 NOUVEAU MEILLEUR MODÈLE!\n   PSNR: 27.5889 dB (Époque 60)\n   SSIM: 0.8476\n   �� Sauvegardé: best_mprnet_model.pth\n\n📊 DÉTAILS DE L'ÉPOQUE  60/100\n   Loss d'entraînement: 0.058193\n   Learning rate: 7.27e-05\n   Temps écoulé: 116.8 minutes\n   Progrès: 60.0%\n============================================================\nEpoch  61/100 | Batch    0 | Loss: 0.067217\nEpoch  61/100 | Batch   50 | Loss: 0.052042\nEpoch  61/100 | Batch  100 | Loss: 0.066012\nEpoch  62/100 | Batch    0 | Loss: 0.040540\nEpoch  62/100 | Batch   50 | Loss: 0.058895\nEpoch  62/100 | Batch  100 | Loss: 0.065524\nEpoch  63/100 | Batch    0 | Loss: 0.052825\nEpoch  63/100 | Batch   50 | Loss: 0.058602\nEpoch  63/100 | Batch  100 | Loss: 0.060556\nEpoch  64/100 | Batch    0 | Loss: 0.063292\nEpoch  64/100 | Batch   50 | Loss: 0.055688\nEpoch  64/100 | Batch  100 | Loss: 0.051248\nEpoch  65/100 | Batch    0 | Loss: 0.055028\nEpoch  65/100 | Batch   50 | Loss: 0.067051\nEpoch  65/100 | Batch  100 | Loss: 0.052358\nEpoch  66/100 | Batch    0 | Loss: 0.044979\nEpoch  66/100 | Batch   50 | Loss: 0.064572\nEpoch  66/100 | Batch  100 | Loss: 0.051687\nEpoch  67/100 | Batch    0 | Loss: 0.063915\nEpoch  67/100 | Batch   50 | Loss: 0.056187\nEpoch  67/100 | Batch  100 | Loss: 0.050365\nEpoch  68/100 | Batch    0 | Loss: 0.059776\nEpoch  68/100 | Batch   50 | Loss: 0.058409\nEpoch  68/100 | Batch  100 | Loss: 0.057098\nEpoch  69/100 | Batch    0 | Loss: 0.050997\nEpoch  69/100 | Batch   50 | Loss: 0.058066\nEpoch  69/100 | Batch  100 | Loss: 0.058689\nEpoch  70/100 | Batch    0 | Loss: 0.051040\nEpoch  70/100 | Batch   50 | Loss: 0.059808\nEpoch  70/100 | Batch  100 | Loss: 0.054429\n\n🔍 VALIDATION - Époque 70\n--------------------------------------------------\n🏆 NOUVEAU MEILLEUR MODÈLE!\n   PSNR: 27.6412 dB (Époque 70)\n   SSIM: 0.8478\n   �� Sauvegardé: best_mprnet_model.pth\n\n📊 DÉTAILS DE L'ÉPOQUE  70/100\n   Loss d'entraînement: 0.057908\n   Learning rate: 4.46e-05\n   Temps écoulé: 135.9 minutes\n   Progrès: 70.0%\n============================================================\nEpoch  71/100 | Batch    0 | Loss: 0.059132\nEpoch  71/100 | Batch   50 | Loss: 0.053862\nEpoch  71/100 | Batch  100 | Loss: 0.075749\nEpoch  72/100 | Batch    0 | Loss: 0.046369\nEpoch  72/100 | Batch   50 | Loss: 0.077037\nEpoch  72/100 | Batch  100 | Loss: 0.057625\nEpoch  73/100 | Batch    0 | Loss: 0.053218\nEpoch  73/100 | Batch   50 | Loss: 0.054509\nEpoch  73/100 | Batch  100 | Loss: 0.051869\nEpoch  74/100 | Batch    0 | Loss: 0.060492\nEpoch  74/100 | Batch   50 | Loss: 0.064844\nEpoch  74/100 | Batch  100 | Loss: 0.059414\nEpoch  75/100 | Batch    0 | Loss: 0.048737\nEpoch  75/100 | Batch   50 | Loss: 0.064855\nEpoch  75/100 | Batch  100 | Loss: 0.055024\nEpoch  76/100 | Batch    0 | Loss: 0.067791\nEpoch  76/100 | Batch   50 | Loss: 0.071944\nEpoch  76/100 | Batch  100 | Loss: 0.061034\nEpoch  77/100 | Batch    0 | Loss: 0.056176\nEpoch  77/100 | Batch   50 | Loss: 0.055033\nEpoch  77/100 | Batch  100 | Loss: 0.052526\nEpoch  78/100 | Batch    0 | Loss: 0.045491\nEpoch  78/100 | Batch   50 | Loss: 0.067005\nEpoch  78/100 | Batch  100 | Loss: 0.049957\nEpoch  79/100 | Batch    0 | Loss: 0.052985\nEpoch  79/100 | Batch   50 | Loss: 0.054515\nEpoch  79/100 | Batch  100 | Loss: 0.062573\nEpoch  80/100 | Batch    0 | Loss: 0.060097\nEpoch  80/100 | Batch   50 | Loss: 0.048639\nEpoch  80/100 | Batch  100 | Loss: 0.056220\n\n🔍 VALIDATION - Époque 80\n--------------------------------------------------\n🏆 NOUVEAU MEILLEUR MODÈLE!\n   PSNR: 27.6644 dB (Époque 80)\n   SSIM: 0.8481\n   �� Sauvegardé: best_mprnet_model.pth\n\n📊 DÉTAILS DE L'ÉPOQUE  80/100\n   Loss d'entraînement: 0.057670\n   Learning rate: 2.19e-05\n   Temps écoulé: 155.0 minutes\n   Progrès: 80.0%\n============================================================\nEpoch  81/100 | Batch    0 | Loss: 0.056405\nEpoch  81/100 | Batch   50 | Loss: 0.069682\nEpoch  81/100 | Batch  100 | Loss: 0.056895\nEpoch  82/100 | Batch    0 | Loss: 0.059371\nEpoch  82/100 | Batch   50 | Loss: 0.054831\nEpoch  82/100 | Batch  100 | Loss: 0.051387\nEpoch  83/100 | Batch    0 | Loss: 0.061297\nEpoch  83/100 | Batch   50 | Loss: 0.050874\nEpoch  83/100 | Batch  100 | Loss: 0.064310\nEpoch  84/100 | Batch    0 | Loss: 0.051956\nEpoch  84/100 | Batch   50 | Loss: 0.056346\nEpoch  84/100 | Batch  100 | Loss: 0.054506\nEpoch  85/100 | Batch    0 | Loss: 0.061349\nEpoch  85/100 | Batch   50 | Loss: 0.057542\nEpoch  85/100 | Batch  100 | Loss: 0.060338\nEpoch  86/100 | Batch    0 | Loss: 0.045489\nEpoch  86/100 | Batch   50 | Loss: 0.054856\nEpoch  86/100 | Batch  100 | Loss: 0.055305\nEpoch  87/100 | Batch    0 | Loss: 0.058986\nEpoch  87/100 | Batch   50 | Loss: 0.058190\nEpoch  87/100 | Batch  100 | Loss: 0.054240\nEpoch  88/100 | Batch    0 | Loss: 0.057341\nEpoch  88/100 | Batch   50 | Loss: 0.048603\nEpoch  88/100 | Batch  100 | Loss: 0.058936\nEpoch  89/100 | Batch    0 | Loss: 0.046699\nEpoch  89/100 | Batch   50 | Loss: 0.057315\nEpoch  89/100 | Batch  100 | Loss: 0.051260\nEpoch  90/100 | Batch    0 | Loss: 0.055539\nEpoch  90/100 | Batch   50 | Loss: 0.052782\nEpoch  90/100 | Batch  100 | Loss: 0.055972\n\n🔍 VALIDATION - Époque 90\n--------------------------------------------------\n🏆 NOUVEAU MEILLEUR MODÈLE!\n   PSNR: 27.6779 dB (Époque 90)\n   SSIM: 0.8484\n   �� Sauvegardé: best_mprnet_model.pth\n\n📊 DÉTAILS DE L'ÉPOQUE  90/100\n   Loss d'entraînement: 0.057667\n   Learning rate: 6.88e-06\n   Temps écoulé: 174.1 minutes\n   Progrès: 90.0%\n============================================================\nEpoch  91/100 | Batch    0 | Loss: 0.064363\nEpoch  91/100 | Batch   50 | Loss: 0.054356\nEpoch  91/100 | Batch  100 | Loss: 0.061405\nEpoch  92/100 | Batch    0 | Loss: 0.065334\nEpoch  92/100 | Batch   50 | Loss: 0.065131\nEpoch  92/100 | Batch  100 | Loss: 0.047077\nEpoch  93/100 | Batch    0 | Loss: 0.063391\nEpoch  93/100 | Batch   50 | Loss: 0.049262\nEpoch  93/100 | Batch  100 | Loss: 0.063259\nEpoch  94/100 | Batch    0 | Loss: 0.042387\nEpoch  94/100 | Batch   50 | Loss: 0.055373\nEpoch  94/100 | Batch  100 | Loss: 0.072266\nEpoch  95/100 | Batch    0 | Loss: 0.051684\nEpoch  95/100 | Batch   50 | Loss: 0.055812\nEpoch  95/100 | Batch  100 | Loss: 0.055969\nEpoch  96/100 | Batch    0 | Loss: 0.067021\nEpoch  96/100 | Batch   50 | Loss: 0.065914\nEpoch  96/100 | Batch  100 | Loss: 0.060062\nEpoch  97/100 | Batch    0 | Loss: 0.054722\nEpoch  97/100 | Batch   50 | Loss: 0.067264\nEpoch  97/100 | Batch  100 | Loss: 0.054136\nEpoch  98/100 | Batch    0 | Loss: 0.055408\nEpoch  98/100 | Batch   50 | Loss: 0.056355\nEpoch  98/100 | Batch  100 | Loss: 0.060429\nEpoch  99/100 | Batch    0 | Loss: 0.059645\nEpoch  99/100 | Batch   50 | Loss: 0.061360\nEpoch  99/100 | Batch  100 | Loss: 0.060332\nEpoch 100/100 | Batch    0 | Loss: 0.048581\nEpoch 100/100 | Batch   50 | Loss: 0.062160\nEpoch 100/100 | Batch  100 | Loss: 0.066217\n\n🔍 VALIDATION - Époque 100\n--------------------------------------------------\n🏆 NOUVEAU MEILLEUR MODÈLE!\n   PSNR: 27.6793 dB (Époque 100)\n   SSIM: 0.8484\n   �� Sauvegardé: best_mprnet_model.pth\n\n📊 DÉTAILS DE L'ÉPOQUE 100/100\n   Loss d'entraînement: 0.057624\n   Learning rate: 1.05e-06\n   Temps écoulé: 193.1 minutes\n   Progrès: 100.0%\n============================================================\n\n🎉 ENTRAÎNEMENT TERMINÉ!\n============================================================\n⏱️  Temps total: 193.1 minutes\n🏆 MEILLEUR MODÈLE (Époque 100):\n   PSNR: 27.6793 dB\n   SSIM: 0.8484\n💾 Fichiers sauvegardés:\n   - Meilleur modèle: best_mprnet_model.pth\n   - Modèle final: mprnet_final_model.pth\n\n📊 STATISTIQUES D'ENTRAÎNEMENT:\n   Époques totales: 100\n   Validations effectuées: 11\n   Époques de validation: [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n   Batches par époque: 132\n   Total de batches: 13200\n   Loss finale: 0.057624\n\n📈 ÉVOLUTION DES PERFORMANCES:\n      Époque   1: PSNR 20.9043 dB, SSIM 0.6524\n      Époque  10: PSNR 26.1906 dB, SSIM 0.8273\n      Époque  20: PSNR 26.9446 dB, SSIM 0.8392\n      Époque  30: PSNR 27.2688 dB, SSIM 0.8434\n      Époque  40: PSNR 27.4531 dB, SSIM 0.8458\n      Époque  50: PSNR 27.5524 dB, SSIM 0.8470\n      Époque  60: PSNR 27.5889 dB, SSIM 0.8476\n      Époque  70: PSNR 27.6412 dB, SSIM 0.8478\n      Époque  80: PSNR 27.6644 dB, SSIM 0.8481\n      Époque  90: PSNR 27.6779 dB, SSIM 0.8484\n   🏆 Époque 100: PSNR 27.6793 dB, SSIM 0.8484\n\n✅ Entraînement terminé avec succès!\n🏆 Meilleures performances: PSNR 27.6793 dB, SSIM 0.8484\n🎯 Meilleur modèle obtenu à l'époque 100\n📁 Modèles sauvegardés pour téléchargement\n","output_type":"stream"}],"execution_count":9}]}